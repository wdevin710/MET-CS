vv1 %*% LL1 %*% t(vv1) # Reproduce A1
A1 <- matrix(c(13, -4, 2, -4, 11, -2, 2, -2, 8), 3, 3, byrow=TRUE)
ev1 <- eigen(A1)
ll1 <- ev1$values # 17  8  7
LL1 <- diag(1, length(ll1))*ll1
vv1 <- ev1$vectors
zapsmall(crossprod(vv1)) # Orthonormality
crossprod(vv1[,1],vv1[,1]) # Orthonormality Cross Product
vv1[,1] %*% vv1[,1] # Unit Length Dot Product
vv1[,1] %*% vv1[,2] # Normal => Dot Product %*%
vv1 %*% LL1 %*% t(vv1) # Reproduce A1
A
A1
yy <- A1
pp <- pmax(yy,t(yy))
pp
ss <- graph.adjacency(pp, mode = "undirected", weighted = TRUE)
evzz <- evcent(ss)
ll <- evzz$value; vv <- evzz$vector
ll
vv
LL1
vv1
vv1[,1]
vv1[,1]/vv1[1,1]
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Project_Functions.R')
sums <- embedding(corpora, Number.of.Sentences, Number.of.Topics)
sums <- embedding(corpora, Number.of.Sentences, Number.of.Topics)
# 1) Parse doc into sentences
sentences <- stringi::stri_split_boundaries(doc, type = "sentence")[[ 1 ]]
names(sentences) <- seq_along(sentences) # Enumerate Sentences
# 2) Find Dtm (each sentence is a document – a row in Dtm), All possible words are the columns.
dtm <- CreateDtm(sentences, ngram_window = c(1,1), verbose = FALSE, cpus = 2) # Convert sentences to a document term matrix.
# Rows - number of sentences, Columns - all the worsd
dtm <- dtm[ rowSums(dtm) > 2 , ] # Remove any documents with 2 or fewer words
vocab <- intersect(colnames(dtm), colnames(gamma)) #
dtm <- dtm / rowSums(dtm) # Scale word frequency between [0,1]
# Dtm X Gamma - Matrix product of each scaled word frequency (dtm) with the embedded values (gamma).
# Transforms the dtm matrix to number of sentences times the number of topics.
dtm_topic <- dtm[ , vocab ] %*% t(gamma[ , vocab ])
dtm_topic <- as.matrix(dtm_topic) # Each sentence (row) is represented by a vector of k topics.
# 3) Get the pairwise distances between each document (embedded sentence)
e_dist <- CalcHellingerDist(dtm_topic) # e_dist is a square matrix of size as the Num of documents
e_measure <- (1 - e_dist) * 100 # Turn distance into a 0 to 100 measure
diag(e_measure) <- 0 # set diagonal elements to zero (sentences connected to themselves)
# Turn into a nearest-neighbor graph (Select "Number.of.Sentences" highest values)
# Keep connections only to the top "Number.of.Sentences" most similar documents (sentences). This creates non-symmetric matrix.
e_measure1 <- apply(e_measure, 1, function(x){
x[ x < sort(x, decreasing = TRUE)[ Number.of.Sentences ] ] <- 0 # Set to zero all elements except the max "Number.of.Sentences" values per ROW
x
})
# NOTE:  e_measure1 becomes transposed!
e_measure2 <- pmax(e_measure1, t(e_measure1)) # Take elementwise max, to turn e_measure into symmetric adjacency matrix.
e_measure3 <- graph.adjacency(e_measure2, mode = "undirected", weighted = TRUE) # Create a graph from adjacency matrix
ev <- evcent(e_measure3) # Calculate eigenvector centrality
sentences
names(ev$vector)
sentences[ names(ev$vector)[ order(ev$vector, decreasing = TRUE)[ 1:Number.of.Sentences ] ] ]
Number.of.Sentences
result <- sentences[ names(ev$vector)[ order(ev$vector, decreasing = TRUE)[ 1:Number.of.Sentences ] ] ]
result <- result[ order(as.numeric(names(result))) ] # Order sentences by index of appearence
result
paste(result, collapse = " ")
result
doc
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums <- embedding(corpora, Number.of.Sentences, Number.of.Topics) # Generate summary for each Document
sums <- embedding(corpora, Number.of.Sentences, Number.of.Topics) # Generate summary for each Document
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Project_Functions.R')
sums <- embedding(corpora, Number.of.Sentences, Number.of.Topics) # Generate summary for each Document
dim(tcm)
corpora$content
dim(gamma)
length(doc.content)
length(doc)
Another Example
A1 <- matrix(c(13, -4, 2, -4, 11, -2, 2, -2, 8), 3, 3, byrow=TRUE)
ev1 <- eigen(A1)
ll1 <- ev1$values # 17  8  7
LL1 <- diag(1, length(ll1))*ll1
vv1 <- ev1$vectors
zapsmall(crossprod(vv1)) # Orthonormality
crossprod(vv1[,1],vv1[,1]) # Orthonormality Cross Product
vv1[,1] %*% vv1[,1] # Unit Length Dot Product
vv1[,1] %*% vv1[,2] # Normal => Dot Product %*%
vv1 %*% LL1 %*% t(vv1) # Reproduce A1
pp <- pmax(yy,t(yy))
ss <- graph.adjacency(pp, mode = "undirected", weighted = TRUE)
evzz <- evcent(ss)
ll <- evzz$value; vv <- evzz$vector
yy <- A1
pp <- pmax(yy,t(yy))
ss <- graph.adjacency(pp, mode = "undirected", weighted = TRUE)
evzz <- evcent(ss)
ll <- evzz$value; vv <- evzz$vector
ll
vv
order(ev$vector, decreasing = TRUE)
order(vv, decreasing = TRUE)
rm(list=ls()); cat("\014") # Clear Workspace and Console
library(tm)
library(textmineR); library(igraph)
source("Project_Functions.R")
rm(list=ls()); cat("\014") # Clear Workspace and Console
library(tm)
library(textmineR); library(igraph)
source("Project_Functions.R")
myFiles <- normalizePath(list.files(path = "data", pattern = "txt",  full.names = TRUE))
Docs.corpus <- Corpus(URISource(myFiles), readerControl=list(reader=readPlain))
dim(Docs.corpus)
Docs.corpus[[1]]$content
inspect(Docs.corpus$TED_1.txt)
Docs.corpus$TED_1.txt
myFiles
myFiles <- normalizePath(list.files(path = "data", pattern = "txt",  full.names = TRUE))
Docs.corpus <- Corpus(URISource(myFiles), readerControl=list(reader=readPlain))
myFiles
Docs.corpus$TED_1.txt
Docs.corpus[[1]]$TED_1.txt
Docs.corpus[[1]]
Docs.corpus[1]
Docs.corpus[1]$TED_1.txt[[1]]
Docs.corpus[1]$TED_1.txt
content(Docs.corpus)
content(Docs.corpus[[1]])
corpora$id <- seq_along(corpora$content) # enumerate documents with id
myFiles <- normalizePath(list.files(path = "data", pattern = "txt",  full.names = TRUE))
Docs.corpus <- Corpus(URISource(myFiles), readerControl=list(reader=readPlain))
corpora <- data.frame()
for (ff in 1:length(Docs.corpus)) {
doc.content <- Docs.corpus[[ff]]$content
temp <- paste(doc.content, collapse = ""); temp <- iconv(temp,"WINDOWS-1252","UTF-8")
temp1 <- data.frame(content=temp, stringsAsFactors = FALSE)
corpora <- rbind(corpora, temp1)
}
corpora
# Find summary for each Doc.
Number.of.Sentences <- 5 # How many sentences will the summary have
Number.of.Topics <- 3 # Number of topics per document.
sums <- embedding(corpora, Number.of.Sentences, Number.of.Topics) # Generate summary for each Document
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
myFiles
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
rm(list=ls()); cat("\014") # Clear Workspace and Console
library(tm)
library(textmineR); library(igraph)
source("Project_Functions.R")
# ====
# Load the PDF data
# pdf.loc <- file.path("data") # folder "PDF Files" with PDFs
# myFiles <- normalizePath(list.files(path = pdf.loc, pattern = "pdf",  full.names = TRUE)) # Get the path (chr-vector) of PDF file names
# # Extract content from PDF files
# Docs.corpus <- Corpus(URISource(myFiles), readerControl = list(reader = readPDF(engine = "xpdf")))
# ====
# Load TED Talks Data
myFiles <- normalizePath(list.files(path = "data", pattern = "rtf",  full.names = TRUE))
Docs.corpus <- Corpus(URISource(myFiles), readerControl=list(reader=readPlain))
myFiles
myFiles <- normalizePath(list.files(path = "data", pattern = "rtf",  full.names = TRUE))
Docs.corpus <- Corpus(URISource(myFiles), readerControl=list(reader=readPlain))
# Extract content of each document in a format needed by textmineR to find text summary
corpora <- data.frame()
for (ff in 1:length(Docs.corpus)) {
doc.content <- Docs.corpus[[ff]]$content
temp <- paste(doc.content, collapse = ""); temp <- iconv(temp,"WINDOWS-1252","UTF-8")
temp1 <- data.frame(content=temp, stringsAsFactors = FALSE)
corpora <- rbind(corpora, temp1)
}
corpora$id <- seq_along(corpora$content) # enumerate documents with id
corpora$content
?iconv
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
corpora$content
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
embedding(corpora, Number.of.Sentences, Number.of.Topics)
embedding(corpora, Number.of.Sentences, Number.of.Topics)
embedding(corpora, Number.of.Sentences, Number.of.Topics)
embedding(corpora, Number.of.Sentences, Number.of.Topics)
embedding(corpora, Number.of.Sentences, Number.of.Topics)
length(doc)
length(sentences)
sentences
sentences[16]
sentences[17]
dtm_topic
e_measure1
e_measure3
ev$vector
names(ev$vector)[ order(ev$vector, decreasing = TRUE)[ 1:Number.of.Sentences ] ]
ev$value
names(ev$vector)[ order(ev$vector, decreasing = TRUE)[ 1:Number.of.Sentences ] ]
order(ev$vector, decreasing = TRUE)[ 1:Number.of.Sentences ]
sort(ev$vector, decreasing = TRUE)
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
dtm_topic
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
corpora$content
doc
gamma
gamma
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
gamma
?FitLdaModel
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
tcm <- CreateTcm(doc_vec = corpora$content,
skipgram_window = 10, # 5-word window on each side
verbose = FALSE,
cpus = 2)
set.seed(123)
embeddings <- FitLdaModel(dtm = tcm,
k = Number.of.Topics, # Integer number of topics
iterations = 200,
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = FALSE,
calc_coherence = FALSE,
calc_r2 = FALSE,
cpus = 2)
embeddings[1]
embeddings[1,1]
embeddings[[1]]
embeddings[[1]][1]
embeddings[[1]][1,]
embeddings[[1]][,1]
head(embeddings)
embeddings[,1]
embeddings[1][,1]
embeddings[[1]][,1]
names(embeddings[[1]][,1])
embeddings
gamma = embeddings$gamma
gamma[,1]
gamma
names(gamma[,1])
names(gamma)[1]
gamma[,1]
gamma
tcm <- CreateTcm(doc_vec = corpora$content,
skipgram_window = 10, # 5-word window on each side
verbose = FALSE,
cpus = 2)
# 2) Create embedding model. Assign probability to each word using Latent Dirichlet Allocation(LDA) and embed words in k topics.
# This function uses Gibbs sampling creating different embeding each run
set.seed(123)
embeddings <- FitLdaModel(dtm = tcm,
k = Number.of.Topics, # Integer number of topics
iterations = 200,
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = FALSE,
calc_coherence = FALSE,
calc_r2 = FALSE,
cpus = 2)
doc <- corpora$content
names(doc) <- corpora$id
gamma = embeddings$gamma
gamma[,1]
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
?set.seed
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Project_Functions.R')
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Project_Functions.R')
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Project_Functions.R')
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Project_Functions.R')
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
gamma[,1]
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
tcm <- CreateTcm(doc_vec = corpora$content,
skipgram_window = 10, # 5-word window on each side
verbose = FALSE,
cpus = 2)
# 2) Create embedding model. Assign probability to each word using Latent Dirichlet Allocation(LDA) and embed words in k topics.
# This function uses Gibbs sampling creating different embeding each run if you don't use set.seed!
set.seed(123) # Keep this value to create simulated values that are reproducible.
embeddings <- FitLdaModel(dtm = tcm,
k = Number.of.Topics, # Integer number of topics
iterations = 200,
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = FALSE,
calc_coherence = FALSE,
calc_r2 = FALSE,
cpus = 2)
doc <- corpora$content
names(doc) <- corpora$id
gamma = embeddings$gamma
sentences <- stringi::stri_split_boundaries(doc, type = "sentence")[[ 1 ]]
names(sentences) <- seq_along(sentences) # Enumerate Sentences
# 2) Find Dtm (each sentence is a document – a row in Dtm), All possible words are the columns.
dtm <- CreateDtm(sentences, ngram_window = c(1,1), verbose = FALSE, cpus = 2) # Convert sentences to a document term matrix.
# Rows - number of sentences, Columns - all the worsd
dtm <- dtm[ rowSums(dtm) > 2 , ] # Remove any documents with 2 or fewer words
vocab <- intersect(colnames(dtm), colnames(gamma)) #
dtm <- dtm / rowSums(dtm) # Scale word frequency between [0,1]
# Dtm X Gamma - Matrix product of each scaled word frequency (dtm) with the embedded values (gamma).
# Transforms the dtm matrix to number of sentences times the number of topics.
dtm_topic <- dtm[ , vocab ] %*% t(gamma[ , vocab ])
dtm_topic <- as.matrix(dtm_topic) # Each sentence (row) is represented by a vector of k topics.
# 3) Get the pairwise distances between each document (embedded sentence)
e_dist <- CalcHellingerDist(dtm_topic) # e_dist is a square matrix of size as the Num of documents
e_measure <- (1 - e_dist) * 100 # Turn distance into a 0 to 100 measure
diag(e_measure) <- 0
rm(list=ls()); cat("\014") # Clear Workspace and Console
library(tm)
library(textmineR); library(igraph)
source("Project_Functions.R")
# ====
# Load the PDF data
# pdf.loc <- file.path("data") # folder "PDF Files" with PDFs
# myFiles <- normalizePath(list.files(path = pdf.loc, pattern = "pdf",  full.names = TRUE)) # Get the path (chr-vector) of PDF file names
# # Extract content from PDF files
# Docs.corpus <- Corpus(URISource(myFiles), readerControl = list(reader = readPDF(engine = "xpdf")))
# ====
# Load TED Talks Data
myFiles <- normalizePath(list.files(path = "data", pattern = "txt",  full.names = TRUE))
Docs.corpus <- Corpus(URISource(myFiles), readerControl=list(reader=readPlain))
# Extract content of each document in a format needed by textmineR to find text summary
corpora <- data.frame()
for (ff in 1:length(Docs.corpus)) {
doc.content <- Docs.corpus[[ff]]$content
temp <- paste(doc.content, collapse = ""); #temp <- iconv(temp,"WINDOWS-1252","UTF-8")
temp1 <- data.frame(content=temp, stringsAsFactors = FALSE)
corpora <- rbind(corpora, temp1)
}
corpora$id <- seq_along(corpora$content) # enumerate documents with id
# Find summary for each Doc.
Number.of.Sentences <- 5 # How many sentences will the summary have
Number.of.Topics <- 3 # Number of topics per document.
sums <- embedding(corpora, Number.of.Sentences, Number.of.Topics) # Generate summary for each Document
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Project_Functions.R')
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
sentences <- stringi::stri_split_boundaries(doc, type = "sentence")[[ 1 ]]
names(sentences) <- seq_along(sentences) # Enumerate Sentences
# 2) Find Dtm (each sentence is a document – a row in Dtm), All possible words are the columns.
dtm <- CreateDtm(sentences, ngram_window = c(1,1), verbose = FALSE, cpus = 2) # Convert sentences to a document term matrix.
# Rows - number of sentences, Columns - all the worsd
dtm <- dtm[ rowSums(dtm) > 2 , ] # Remove any documents with 2 or fewer words
vocab <- intersect(colnames(dtm), colnames(gamma)) #
dtm <- dtm / rowSums(dtm) # Scale word frequency between [0,1]
# Dtm X Gamma - Matrix product of each scaled word frequency (dtm) with the embedded values (gamma).
# Transforms the dtm matrix to number of sentences times the number of topics.
dtm_topic <- dtm[ , vocab ] %*% t(gamma[ , vocab ])
dtm_topic <- as.matrix(dtm_topic) # Each sentence (row) is represented by a vector of k topics.
# 3) Get the pairwise distances between each document (embedded sentence)
e_dist <- CalcHellingerDist(dtm_topic) # e_dist is a square matrix of size as the Num of documents
e_measure <- (1 - e_dist) * 100 # Turn distance into a 0 to 100 measure
diag(e_measure) <- 0 # set diagonal elements to zero (sentences connected to themselves)
# Turn into a nearest-neighbor graph (Select "Closest.Neighbors" highest values)
# Keep connections only to the top "Closest.Neighbors" most similar documents (sentences). This creates non-symmetric matrix.
Closest.Neighbors = 5
e_measure1 <- apply(e_measure, 1, function(x){
x[ x < sort(x, decreasing = TRUE)[ Closest.Neighbors ] ] <- 0 # Set to zero all elements except the max "Number.of.Sentences" values per ROW
x
})
# NOTE:  e_measure1 becomes transposed!
debugSource('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
e_measure3 <- graph.adjacency(e_measure1, mode = "undirected", weighted = TRUE)
ev <- evcent(e_measure3)
ev$options
ev$value
result <- sentences[ names(ev$vector)[ order(ev$vector, decreasing = TRUE)[ 1:Number.of.Sentences ] ] ]
result <- result[ order(as.numeric(names(result))) ] # Order sentences by index of appearence
result <- paste(result, collapse = " ")
result
e_measure1
z <- eigen(e_measure1)
z$values
max(z$values)
z <- eigen(e_measure2)
z$values
ev$value
sort(z$values)
z$vectors
ev$value
z$values
z$vectors[1]
z$vectors[1,]
z$vectors[1,]
z$vectors[,1]
ev$vector
z <- eigen(e_measure3)
dim
(e_measure3)
z <- eigen(e_measure2)
z$values
ev$value
z$vectors(,1)
z$vectors[,1]
ev$vector
names(ev$vector)[ order(ev$vector, decreasing = TRUE)[ 1:Number.of.Sentences ] ]
abs(z$vectors[,1])
abs(z$vectors[,1])
names(abs(z$vectors[,1]))[ order(abs(z$vectors[,1]), decreasing = TRUE)[ 1:Number.of.Sentences ] ]
order(abs(z$vectors[,1]), decreasing = TRUE)
e_measure3 <- graph.adjacency(e_measure1, mode = "undirected", weighted = TRUE)
z <- eigen(e_measure1)
z$values
ev <- evcent(e_measure3)
ev$value
sentences[ names(ev$vector)[ order(ev$vector, decreasing = TRUE)[ 1:Number.of.Sentences ] ] ]
names(ev$vector)[ order(ev$vector, decreasing = TRUE)[ 1:Number.of.Sentences ] ]
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Project_Functions.R')
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
source('~/Documents/My Files/MyCode/Spring 2019/Class 6/Document Summarization/Summarize.R')
sums
rm(list=ls()); cat("\014") # Clear Workspace and Console
library(tm)
library(textmineR); library(igraph)
source("Project_Functions.R")
myFiles <- normalizePath(list.files(path = "data", pattern = "txt",  full.names = TRUE))
Docs.corpus <- Corpus(URISource(myFiles), readerControl=list(reader=readPlain))
corpora <- data.frame()
for (ff in 1:length(Docs.corpus)) {
doc.content <- Docs.corpus[[ff]]$content
temp <- paste(doc.content, collapse = ""); #temp <- iconv(temp,"WINDOWS-1252","UTF-8")
temp1 <- data.frame(content=temp, stringsAsFactors = FALSE)
corpora <- rbind(corpora, temp1)
}
tcm <- CreateTcm(doc_vec = corpora$content,
skipgram_window = 10, # 5-word window on each side
verbose = FALSE,
cpus = 2)
?CreateTcm
tcm[1,1:10]
tcm[1:5,1:10]
tcm[1:5,1:7]
set.seed(123) # Keep this value to create simulated values that are reproducible.
embeddings <- FitLdaModel(dtm = tcm,
k = Number.of.Topics, # Integer number of topics
iterations = 200,
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = FALSE,
calc_coherence = FALSE,
calc_r2 = FALSE,
cpus = 2)
doc <- corpora$content
names(doc) <- corpora$id
gamma = embeddings$gamma
corpora$id <- seq_along(corpora$content) # enumerate documents with id
# Find summary for each Doc.
Number.of.Sentences <- 7 # How many sentences will the summary have
Number.of.Topics <- 5
set.seed(123) # Keep this value to create simulated values that are reproducible.
embeddings <- FitLdaModel(dtm = tcm,
k = Number.of.Topics, # Integer number of topics
iterations = 200,
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = FALSE,
calc_coherence = FALSE,
calc_r2 = FALSE,
cpus = 2)
doc <- corpora$content
names(doc) <- corpora$id
gamma = embeddings$gamma
sentences <- stringi::stri_split_boundaries(doc, type = "sentence")[[ 1 ]]
names(sentences) <- seq_along(sentences) # Enumerate Sentences
# 2) Find Dtm (each sentence is a document – a row in Dtm), All possible words are the columns.
dtm <- CreateDtm(sentences, ngram_window = c(1,1), verbose = FALSE, cpus = 2) # Convert sentences to a document term matrix.
# Rows - number of sentences, Columns - all the worsd
dtm <- dtm[ rowSums(dtm) > 2 , ] # Remove any documents with 2 or fewer words
vocab <- intersect(colnames(dtm), colnames(gamma)) #
source('~/Documents/My Files/MyClass/2021/Text_Summarization/Document Summarization/Summarize.R', echo=TRUE)
