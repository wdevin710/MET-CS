# read XML File located in folder "pth"
x = xmlParse(file.path(pth,"sequencing.xml"))
### --- Example 6: Search Wikipedia web pages. --------
# Save these 3 files separately in the same folder (Related to HW#4)
# Example: Shiny app that search Wikipedia web pages
# File: ui.R
library(shiny)
titles <- c("Web_analytics","Text_mining","Integral", "Calculus",
"Lists_of_integrals", "Derivative","Alternating_series",
"Pablo_Picasso","Vincent_van_Gogh","Lev_Tolstoj","Web_crawler")
# Define UI for application
shinyUI(fluidPage(
# Application title (Panel 1)
titlePanel("Wiki Pages"),
# Widget (Panel 2)
sidebarLayout(
sidebarPanel(h3("Search panel"),
# Where to search
selectInput("select",
label = h5("Choose from the following Wiki Pages on"),
choices = titles,
selected = titles, multiple = TRUE),
# Start Search
submitButton("Results")
),
# Display Panel (Panel 3)
mainPanel(
h1("Display Panel",align = "center"),
plotOutput("distPlot")
)
)
))
# Wikipedia Search
library(tm)
library(stringi)
library(WikipediR)
SearchWiki <- function (titles) {
# wiki.URL <- "https://en.wikipedia.org/wiki/"
# articles <- lapply(titles,function(i) stri_flatten(readLines(stri_paste(wiki.URL,i)), col = " "))
articles <- lapply(titles,function(i) page_content("en","wikipedia", page_name = i,as_wikitext=TRUE)$parse$wikitext)
docs <- Corpus(VectorSource(articles)) # Get Web Pages' Corpus
remove(articles)
# Text analysis - Preprocessing
transform.words <- content_transformer(function(x, from, to) gsub(from, to, x))
temp <- tm_map(docs, transform.words, "<.+?>", " ")
temp <- tm_map(temp, transform.words, "\t", " ")
temp <- tm_map(temp, content_transformer(tolower)) # Conversion to Lowercase
# temp <- tm_map(temp, PlainTextDocument)
temp <- tm_map(temp, stripWhitespace)
temp <- tm_map(temp, removeWords, stopwords("english"))
temp <- tm_map(temp, removePunctuation)
temp <- tm_map(temp, stemDocument, language = "english") # Perform Stemming
remove(docs)
# Create Dtm
dtm <- DocumentTermMatrix(temp)
dtm <- removeSparseTerms(dtm, 0.4)
dtm$dimnames$Docs <- titles
docsdissim <- dist(as.matrix(dtm), method = "euclidean") # Distance Measure
h <- hclust(as.dist(docsdissim), method = "ward.D2") # Group Results
}
# File: server.R
library(shiny)
library(tm)
library(quantmod)
# Define server logic required to implement search
shinyServer(function(input, output) {
output$text1 <- renderUI({
Str1 <- paste("You have searched for Quotes on:", input$text.Search)
result <- getQuote(input$text.Search, what=yahooQF("Last Trade (Price Only)"))
dataOutput <- paste("<li>",strong(result),"</li>")
Str2 <- "Search Results:"
HTML(paste(Str1, Str2, dataOutput, sep = '<br/>'))
})
})
library(shiny)
library(tm)
library(stringi)
# library(proxy)
source("WikiSearch.R")
# library(proxy)
source("WikiSearch.R")
getwd()
setwd("~/Desktop/Boston University Graduate Study/2022 SPRING/MET CS 688/Data")
# library(proxy)
source("WikiSearch.R")
shinyServer(function(input, output) {
output$distPlot <- renderPlot({
result <- SearchWiki(input$select)
plot(result, labels = input$select, sub = "",main="Wikipedia Search")
})
})
library(shiny)
titles <- c("Web_analytics","Text_mining","Integral", "Calculus",
"Lists_of_integrals", "Derivative","Alternating_series",
"Pablo_Picasso","Vincent_van_Gogh","Lev_Tolstoj","Web_crawler")
# Define UI for application
shinyUI(fluidPage(
# Application title (Panel 1)
titlePanel("Wiki Pages"),
# Widget (Panel 2)
sidebarLayout(
sidebarPanel(h3("Search panel"),
# Where to search
selectInput("select",
label = h5("Choose from the following Wiki Pages on"),
choices = titles,
selected = titles, multiple = TRUE),
# Start Search
submitButton("Results")
),
# Display Panel (Panel 3)
mainPanel(
h1("Display Panel",align = "center"),
plotOutput("distPlot")
)
)
))
library(shiny)
# Define UI for application
shinyUI(fluidPage(
# Application title (Panel 1)
titlePanel("Quotes Search App"),
# Widget (Panel 2)
sidebarLayout(
sidebarPanel(h3("Search panel"),
# Search for
textInput("text.Search", label = h5("Get quotes for"),
value = "AAPL"),
# Start Search
submitButton("Results")
),
# Display Panel (Panel 3)
mainPanel(
h1("Display Panel",align = "center"),
htmlOutput("text1")
)
)
))
library(shiny)
titles <- c("Web_analytics","Text_mining","Integral", "Calculus",
"Lists_of_integrals", "Derivative","Alternating_series",
"Pablo_Picasso","Vincent_van_Gogh","Lev_Tolstoj","Web_crawler")
# Define UI for application
shinyUI(fluidPage(
# Application title (Panel 1)
titlePanel("Wiki Pages"),
# Widget (Panel 2)
sidebarLayout(
sidebarPanel(h3("Search panel"),
# Where to search
selectInput("select",
label = h5("Choose from the following Wiki Pages on"),
choices = titles,
selected = titles, multiple = TRUE),
# Start Search
submitButton("Results")
),
# Display Panel (Panel 3)
mainPanel(
h1("Display Panel",align = "center"),
plotOutput("distPlot")
)
)
))
# File: ui.R
library(shiny)
# Define UI for application
shinyUI(fluidPage(
# Application title (Panel 1)
titlePanel("Quotes Search App"),
# Widget (Panel 2)
sidebarLayout(
sidebarPanel(h3("Search panel"),
# Search for
textInput("text.Search", label = h5("Get quotes for"),
value = "AAPL"),
# Start Search
submitButton("Results")
),
# Display Panel (Panel 3)
mainPanel(
h1("Display Panel",align = "center"),
htmlOutput("text1")
)
)
))
# File: server.R
library(shiny)
library(tm)
library(quantmod)
# Define server logic required to implement search
shinyServer(function(input, output) {
output$text1 <- renderUI({
Str1 <- paste("You have searched for Quotes on:", input$text.Search)
result <- getQuote(input$text.Search, what=yahooQF("Last Trade (Price Only)"))
dataOutput <- paste("<li>",strong(result),"</li>")
Str2 <- "Search Results:"
HTML(paste(Str1, Str2, dataOutput, sep = '<br/>'))
})
})
library(shiny)
library(tm)
library(quantmod)
shinyServer(function(input, output) {
output$text1 <- renderUI({
Str1 <- paste("You have searched for Quotes on:", input$text.Search)
result <- getQuote(input$text.Search, what=yahooQF("Last Trade (Price Only)"))
dataOutput <- paste("<li>",strong(result),"</li>")
Str2 <- "Search Results:"
HTML(paste(Str1, Str2, dataOutput, sep = '<br/>'))
})
})
shinyServer(1,2)
runApp()
runApp('Shiny')
SearchWiki <- function (titles) {
# wiki.URL <- "https://en.wikipedia.org/wiki/"
# articles <- lapply(titles,function(i) stri_flatten(readLines(stri_paste(wiki.URL,i)), col = " "))
articles <- lapply(titles,function(i) page_content("en","wikipedia", page_name = i,as_wikitext=TRUE)$parse$wikitext)
docs <- Corpus(VectorSource(articles)) # Get Web Pages' Corpus
remove(articles)
# Text analysis - Preprocessing
transform.words <- content_transformer(function(x, from, to) gsub(from, to, x))
temp <- tm_map(docs, transform.words, "<.+?>", " ")
temp <- tm_map(temp, transform.words, "\t", " ")
temp <- tm_map(temp, content_transformer(tolower)) # Conversion to Lowercase
# temp <- tm_map(temp, PlainTextDocument)
temp <- tm_map(temp, stripWhitespace)
temp <- tm_map(temp, removeWords, stopwords("english"))
temp <- tm_map(temp, removePunctuation)
temp <- tm_map(temp, stemDocument, language = "english") # Perform Stemming
remove(docs)
# Create Dtm
dtm <- DocumentTermMatrix(temp)
dtm <- removeSparseTerms(dtm, 0.4)
dtm$dimnames$Docs <- titles
docsdissim <- dist(as.matrix(dtm), method = "euclidean") # Distance Measure
h <- hclust(as.dist(docsdissim), method = "ward.D2") # Group Results
}
runApp('~/Desktop/Boston University Graduate Study/2022 SPRING/MET CS 688/Data')
setwd("~/Desktop/Boston University Graduate Study/2022 SPRING/MET CS 688/Data")
runApp('~/Desktop/Boston University Graduate Study/2022 SPRING/MET CS 688/Data')
runApp('~/Desktop/Boston University Graduate Study/2022 SPRING/MET CS 688/Data')
library(tm)
library(stringi)
library(WikipediR)
SearchWiki <- function (titles) {
# wiki.URL <- "https://en.wikipedia.org/wiki/"
# articles <- lapply(titles,function(i) stri_flatten(readLines(stri_paste(wiki.URL,i)), col = " "))
articles <- lapply(titles,function(i) page_content("en","wikipedia", page_name = i,as_wikitext=TRUE)$parse$wikitext)
docs <- Corpus(VectorSource(articles)) # Get Web Pages' Corpus
remove(articles)
# Text analysis - Preprocessing
transform.words <- content_transformer(function(x, from, to) gsub(from, to, x))
temp <- tm_map(docs, transform.words, "<.+?>", " ")
temp <- tm_map(temp, transform.words, "\t", " ")
temp <- tm_map(temp, content_transformer(tolower)) # Conversion to Lowercase
# temp <- tm_map(temp, PlainTextDocument)
temp <- tm_map(temp, stripWhitespace)
temp <- tm_map(temp, removeWords, stopwords("english"))
temp <- tm_map(temp, removePunctuation)
temp <- tm_map(temp, stemDocument, language = "english") # Perform Stemming
remove(docs)
# Create Dtm
dtm <- DocumentTermMatrix(temp)
dtm <- removeSparseTerms(dtm, 0.4)
dtm$dimnames$Docs <- titles
docsdissim <- dist(as.matrix(dtm), method = "euclidean") # Distance Measure
h <- hclust(as.dist(docsdissim), method = "ward.D2") # Group Results
}
runApp('~/Desktop/Boston University Graduate Study/2022 SPRING/MET CS 688/Data')
runApp()
runApp()
runApp()
install.packages("wordcloud")
install.packages("memoise")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(proxy)
runApp()
runApp()
runApp()
runApp()
library(wordcloud)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
dtm <- removeSparseTerms(dtm, 0.4)
runApp()
runApp()
runApp()
runApp()
runApp()
m <- sort(colSums(m) ,decreasing = TRUE)
runApp()
12/(12+15)
Precision = TruePositives / (TruePositives + FalsePositives)
12/(12+15)
reut21578 <- system.file("texts", "acq", package = "tm") #
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
library(tm)
reut21578 <- system.file("texts", "acq", package = "tm") #
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
reuters
#12
reut21578 <- system.file("texts", "acq", package = "tm") #
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
dtm <- as.matrix(DocumentTermMatrix(reuters))
dtm
View(dtm)
View(dtm)
sort(colSums(dtm), decreasing = TRUE)
dtm <- sort(colSums(dtm), decreasing = TRUE)
dtm <- dtm[1:2]
dtm
dtm <- dtm[1:3]
dtm
dtm <- dtm[1:4]
dtm
dtm <- as.matrix(DocumentTermMatrix(reuters))
dtm <- sort(colSums(dtm), decreasing = TRUE)
dtm <- dtm[1:4]
dtm
dtm <- sort(rowSums(dtm), decreasing = TRUE)
dtm <- as.matrix(DocumentTermMatrix(reuters))
dtm <- sort(rowSums(dtm), decreasing = TRUE)
dtm <- dtm[1:4]
dtm
dtm <- sort(colSums(dtm), decreasing = TRUE)
dtm <- dtm[1:4]
reut21578 <- system.file("texts", "acq", package = "tm") #
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
reuters
nchar(reuters)
dtm <- as.matrix(DocumentTermMatrix(reuters[,13]))
dtm <- as.matrix(DocumentTermMatrix(reuters[13,]))
dtm <- as.matrix(DocumentTermMatrix(reuters[14,]))
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
a <- subset(reuters,14)
View(reuters)
a <- reuter[14]
a <- reuters[14]
dtm <- as.matrix(DocumentTermMatrix(a))
dtm
colSums(dtm)
View(a)
View(reuters)
ppois(48, lambda=50)
n <- 20
p <- 25/75
pbinom(9, size = n, prob = p)
n <- 20
p <- 25/75
pbinom(9, size = n, prob = p)
p <- 15/45
pbinom(9, size = n, prob = p)
15/45
pbinom(9, size = n, prob = p)
dbinom(9, size = n, prob = p)
n <- 18
p <- 1/2
dbinom(14, size = n, prob = p)
dbinom(12:n, size = n, prob = p)
sum(dbinom(12:n, size = n, prob = p))
n <- 20
p <- 15/45
dbinom(9, size = n, prob = p)
p <- 0.33
dbinom(9, size = n, prob = p)
p <- 0.333
dbinom(9, size = n, prob = p)
p <- 15/45
dbinom(9, size = n, prob = p)
1-dbinom(9, size = n, prob = p)
dpois(121, lambda=125)
n <- 5
p <- 0.46
dbinom(3, size = n, prob = p)
dnbinom(6, size = 1, prob = 1/6)
1/6
dnbinom(1, size = 1, prob = 1/6)
dnbinom(1, size = 1, prob = 1/6)
n <- 5
p <- 0.22
sum(dbinom(2:n, size = n, prob = p))
dbinom(5, size = n, prob = p)
p <- 0.63
dbinom(5, size = n, prob = p)
p <- 15/45
n <- 20
M <- 15    # Of interest
N <- 30    # Not of interest
K <- 20    # Sample Size
phyper(9, m = M, n = N, k = K)
ppois(48,lamba = 50)
ppois(48,lambda = 50)
n <- 18 ; p <- 1/2
sum(dbinom(12:n, size = n, prob = p))
phyper(9, m = M, n = N, k = K)
dbinom(14, size = n, prob = p)
dpois(121, lambda=125)
n <- 5 ; p <- 0.46
pbinom(3, size = n, prob = p)
#Q7
#Q8
#Q9
n <- 5 ; p <- 0.22
sum(dbinom(2:n, size = n, prob = p))
#Q10
n <- 5 ; p <- 0.63
dbinom(5, size = n, prob = p)
#Q7
#Q8
#Q9
n <- 5 ; p <- 0.22
sum(dbinom(2:n, size = n, prob = p))
1 - pbinom(3, size = n, prob = p)
1 - pbinom(2, size = n, prob = p)
sum(dbinom(2:n, size = n, prob = p))
#Q7
#Q8
#Q9
n <- 5 ; p <- 0.22
sum(dbinom(2:n, size = n, prob = p))
1 - pbinom(2, size = n, prob = p)
1 - pbinom(1, size = n, prob = p)
n <- 5 ; p <- 0.46
pbinom(3, size = n, prob = p)
#
r <- 1
p <- 1/6
sum(dnbinom(0:5, size = r, prob = p))
pnbinom(0, size = r, prob = p)
pnbinom(1, size = r, prob = p)
library(tm)
library(stringi)
library(WikipediR)
library(wordcloud)
SearchWiki <- function (titles) {
# wiki.URL <- "https://en.wikipedia.org/wiki/"
# articles <- lapply(titles,function(i) stri_flatten(readLines(stri_paste(wiki.URL,i)), col = " "))
articles <- lapply(titles,function(i) page_content("en","wikipedia", page_name = i,as_wikitext=TRUE)$parse$wikitext)
docs <- Corpus(VectorSource(articles)) # Get Web Pages' Corpus
remove(articles)
# Text analysis - Preprocessing
transform.words <- content_transformer(function(x, from, to) gsub(from, to, x))
temp <- tm_map(docs, transform.words, "<.+?>", " ")
temp <- tm_map(temp, transform.words, "\t", " ")
temp <- tm_map(temp, content_transformer(tolower)) # Conversion to Lowercase
# temp <- tm_map(temp, PlainTextDocument)
temp <- tm_map(temp, stripWhitespace)
temp <- tm_map(temp, removeWords, stopwords("english"))
temp <- tm_map(temp, removePunctuation)
temp <- tm_map(temp, stemDocument, language = "english") # Perform Stemming
remove(docs)
# Create Dtm
dtm <- DocumentTermMatrix(temp)
dtm <- removeSparseTerms(dtm, 0.4)
dtm$dimnames$Docs <- titles
m <- as.matrix(dtm)
m <- sort(colSums(m) ,decreasing = TRUE)
m <- m[1:50]
}
library(shiny)
library(tm)
library(stringi)
library(wordcloud)
# library(proxy)
source("WikiSearch.R")
shinyServer(function(input, output) {
output$distPlot <- renderPlot({
withProgress({
setProgress(message = "Mining Wikipedia...")
result <- SearchWiki(input$select)
})
wordcloud( names(result), result, scale=c(4,0.9), colors=brewer.pal(6, "Dark2"))
})
})
library(shiny)
titles <- c("Web_analytics","Text_mining","Integral", "Calculus",
"Lists_of_integrals", "Derivative","Alternating_series",
"Pablo_Picasso","Vincent_van_Gogh","Lev_Tolstoj","Web_crawler")
# Define UI for application
shinyUI(fluidPage(
# Application title (Panel 1)
titlePanel("Wiki Pages"),
# Widget (Panel 2)
sidebarLayout(
sidebarPanel(h3("Search panel"),
# Where to search
selectInput("select",
label = h5("Choose from the following Wiki Pages on"),
choices = titles,
selected = titles, multiple = TRUE),
# Start Search
submitButton("Results")
),
# Display Panel (Panel 3)
mainPanel(
h1("Display Panel",align = "center"),
plotOutput("distPlot")
)
)
))
runApp()
